{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX & WIT\n",
    "### Exploring using the What-If Tool with TFX\n",
    "\n",
    "This notebook explores using MLMD payloads from the Chicago Taxi pipeline example, which are created in the TFX developer tutorial, with the What-If Tool.\n",
    "\n",
    "# Something seems wrong\n",
    "I load the SavedModel created by Trainer, which should include the transform graph created by Transform.  It should expect the tf.Examples created by ExampleGen, and the schema from SchemaGen, **but that doesn't work**.  What does work is to give the model the transformed examples and transformed schema from **Transform**.  This suggests to me that **Trainer isn't including the transform graph in the SavedModel that it creates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertcrowe/tfx-env/lib/python3.6/site-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline DB:\n",
      "/Users/robertcrowe/airflow/tfx/metadata/taxi_solution/metadata.db\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, pprint\n",
    "import tensorflow as tf\n",
    "import tfx_utils\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def _make_default_sqlite_uri(pipeline_name):\n",
    "    return os.path.join(os.environ['HOME'], 'airflow/tfx/metadata', pipeline_name, 'metadata.db')\n",
    "\n",
    "def get_metadata_store(pipeline_name):\n",
    "    return tfx_utils.TFXReadonlyMetadataStore.from_sqlite_db(_make_default_sqlite_uri(pipeline_name))\n",
    "\n",
    "pipeline_name = 'taxi_solution' # or taxi_solution\n",
    "pipeline_db_path = _make_default_sqlite_uri(pipeline_name)\n",
    "print('Pipeline DB:\\n{}'.format(pipeline_db_path))\n",
    "\n",
    "store = get_metadata_store(pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the SavedModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldir: /Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Trainer/output/6/serving_model_dir/export/chicago-taxi/1560181362\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "models = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.MODEL)\n",
    "modelroot = os.path.join(models.URI.iloc[0], 'serving_model_dir', 'export', 'chicago-taxi')\n",
    "newest = str(sorted([int(f) for f in listdir(modelroot) if f.isdigit()])[-1])\n",
    "modeldir = os.path.join(modelroot, newest)\n",
    "\n",
    "print('modeldir: {}'.format(modeldir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the feature columns for the Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "_MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "\n",
    "_CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "_FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "_BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "_VOCAB_FEATURE_KEYS = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "_VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "_OOV_SIZE = 10\n",
    "\n",
    "# WEIRD: Since we're using the features created by Transform, we need to change the names to match what Transform names them\n",
    "def _transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def _transformed_names(keys):\n",
    "    return [_transformed_name(key) for key in keys]\n",
    "\n",
    "real_valued_columns = [\n",
    "    tf.feature_column.numeric_column(key, shape=(), default_value=0)\n",
    "    for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    tf.feature_column.categorical_column_with_identity(key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
    "    for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns += [\n",
    "    tf.feature_column.categorical_column_with_identity(\n",
    "        key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
    "    for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns += [\n",
    "    tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
    "        key, num_buckets=num_buckets, default_value=0) for key, num_buckets in zip(\n",
    "        _transformed_names(_CATEGORICAL_FEATURE_KEYS), _MAX_CATEGORICAL_FEATURE_VALUES)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the trained Estimator from the SavedModel saved by Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0616 21:18:27.862417 4543722944 estimator.py:1739] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/20/9s3ttk5x4g97nqj3mg9wmh_r00k8nb/T/tmpsnl4d63d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 21:18:27.869770 4543722944 estimator.py:1760] Using temporary folder as model directory: /var/folders/20/9s3ttk5x4g97nqj3mg9wmh_r00k8nb/T/tmpsnl4d63d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/20/9s3ttk5x4g97nqj3mg9wmh_r00k8nb/T/tmpsnl4d63d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13381b160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0616 21:18:27.872887 4543722944 estimator.py:201] Using config: {'_model_dir': '/var/folders/20/9s3ttk5x4g97nqj3mg9wmh_r00k8nb/T/tmpsnl4d63d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13381b160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting from a SavedModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0616 21:18:27.875342 4543722944 estimator.py:2292] Warm-starting from a SavedModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is a (<class 'tensorflow_estimator.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier'>)\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes in the first layer of the DNN\n",
    "first_dnn_layer_size = 100\n",
    "num_dnn_layers = 4\n",
    "dnn_decay_factor = 0.7\n",
    "\n",
    "hidden_units=[\n",
    "    max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
    "    for i in range(num_dnn_layers)\n",
    "]\n",
    "\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    linear_feature_columns=categorical_columns,\n",
    "    dnn_feature_columns=real_valued_columns,\n",
    "    dnn_hidden_units=hidden_units,\n",
    "    warm_start_from=modeldir)\n",
    "    \n",
    "print('model is a ({})'.format(type(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: Grab the schema created by Transform:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_root: /Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4/\n",
      "\u001b[34mmetadata\u001b[m\u001b[m/             \u001b[34mtransform_fn\u001b[m\u001b[m/         \u001b[34mtransformed_metadata\u001b[m\u001b[m/\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4//metadata:\n",
      "schema.pbtxt\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4//transform_fn:\n",
      "\u001b[34massets\u001b[m\u001b[m/         saved_model.pb  \u001b[34mvariables\u001b[m\u001b[m/\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4//transform_fn/assets:\n",
      "vocab_compute_and_apply_vocabulary_1_vocabulary\n",
      "vocab_compute_and_apply_vocabulary_vocabulary\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4//transform_fn/variables:\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4//transformed_metadata:\n",
      "schema.pbtxt\n",
      "schema_uri: /Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transform_output/4/transformed_metadata/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "transform_root = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.TRANSFORMED_EXAMPLES).iloc[0].URI\n",
    "print('transform_root: {}'.format(transform_root))\n",
    "!ls -RF {transform_root}\n",
    "schema_uri = os.path.join(transform_root, 'transformed_metadata', 'schema.pbtxt')\n",
    "print('schema_uri: {}'.format(schema_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'company_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'dropoff_census_tract_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'dropoff_community_area_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'dropoff_latitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'dropoff_longitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'fare_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'payment_type_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_census_tract_xf': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
      "    'pickup_community_area_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_latitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_longitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'tips_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_miles_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'trip_seconds_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'trip_start_day_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_start_hour_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_start_month_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_transform as tft\n",
    "from tfx.utils import io_utils\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "schema_utils = tft.tf_metadata.schema_utils\n",
    "schema_proto = io_utils.parse_pbtxt_file(file_name=schema_uri, message=schema_pb2.Schema())\n",
    "feature_spec, domains = schema_utils.schema_as_feature_spec(schema_proto)\n",
    "\n",
    "pp.pprint(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: Extract the tf.Examples from the tf.Records file created by Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_root: /Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transformed_examples/4/\n",
      "\u001b[34meval\u001b[m\u001b[m/  \u001b[34mtrain\u001b[m\u001b[m/\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transformed_examples/4//eval:\n",
      "transformed_examples-00000-of-00001.gz\n",
      "\n",
      "/Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transformed_examples/4//train:\n",
      "transformed_examples-00000-of-00001.gz\n",
      "examples_uri: /Users/robertcrowe/airflow/tfx/pipelines/taxi_solution/Transform/transformed_examples/4/eval/transformed_examples-00000-of-00001.gz\n",
      "\n",
      "raw_dataset: (<class 'tensorflow.python.data.ops.readers.TFRecordDatasetV1'>) <TFRecordDatasetV1 shapes: (), types: tf.string>\n",
      "parsed_examples is a: <class 'tensorflow.python.data.ops.readers.TFRecordDatasetV1'>\n"
     ]
    }
   ],
   "source": [
    "examples_root = transform_root.replace('transform_output', 'transformed_examples')\n",
    "print('examples_root: {}'.format(examples_root))\n",
    "!ls -RF {examples_root}\n",
    "examples_uri = os.path.join(examples_root, 'eval', listdir(os.path.join(examples_root, 'eval'))[0])\n",
    "print('examples_uri: {}\\n'.format(examples_uri))\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset([examples_uri], compression_type='GZIP')\n",
    "print('raw_dataset: ({}) {}'.format(type(raw_dataset), raw_dataset))\n",
    "\n",
    "parsed_examples = []\n",
    "for ex in raw_dataset:\n",
    "    ex2 = tf.train.Example.FromString(ex.numpy())\n",
    "    parsed_examples.append(ex2)\n",
    "print('parsed_examples is a: {}'.format(type(raw_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: This should not work.  The model should be expecting the examples from ExampleGen, and the schema from SchemaGen.  We're giving it the transformed examples from Transform, and the transformed schema from Transform.  Something is fishy.**\n",
    "\n",
    "Now analyze the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07eb15259624475ad452809226e7fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'classification', 'label_vocab': ['good_tipper', 'bad_tipper'], 'are_sequence_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/robertcrowe/tfx-env/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 21:40:05.347295 4543722944 deprecation.py:323] From /Users/robertcrowe/tfx-env/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/robertcrowe/tfx-env/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 21:40:05.625062 4543722944 deprecation.py:323] From /Users/robertcrowe/tfx-env/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget\n",
    "\n",
    "tool_height_in_px = 1000\n",
    "\n",
    "# Setup the tool with the test examples and the trained classifier\n",
    "config_builder = WitConfigBuilder(parsed_examples).set_estimator_and_feature_spec(\n",
    "    model, feature_spec).set_label_vocab(['good_tipper', 'bad_tipper'])\n",
    "WitWidget(config_builder, height=tool_height_in_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
