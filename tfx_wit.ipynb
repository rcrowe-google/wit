{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/zhitaoli/tfx-env/lib/python3.5/site-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline DB:\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/metadata/chicago_taxi_simple/metadata.db\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, pprint\n",
    "import tensorflow as tf\n",
    "import tfx_utils\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def _make_default_sqlite_uri(pipeline_name):\n",
    "    return os.path.join('/tmp/tmpxd30uz6t/testSimplePipeline/tfx/metadata', pipeline_name, 'metadata.db')\n",
    "\n",
    "def get_metadata_store(pipeline_name):\n",
    "    return tfx_utils.TFXReadonlyMetadataStore.from_sqlite_db(_make_default_sqlite_uri(pipeline_name))\n",
    "\n",
    "pipeline_name = 'chicago_taxi_simple' # or taxi_solution\n",
    "pipeline_db_path = _make_default_sqlite_uri(pipeline_name)\n",
    "print('Pipeline DB:\\n{}'.format(pipeline_db_path))\n",
    "\n",
    "store = get_metadata_store(pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX & WIT\n",
    "### Exploring using the What-If Tool with TFX\n",
    "\n",
    "This notebook explores using MLMD payloads from the Chicago Taxi pipeline example, which are created in the TFX developer tutorial, with the What-If Tool.\n",
    "\n",
    "# Something seems wrong\n",
    "I load the SavedModel created by Trainer, which should include the transform graph created by Transform.  It should expect the tf.Examples created by ExampleGen, and the schema from SchemaGen, **but that doesn't work**.  What does work is to give the model the transformed examples and transformed schema from **Transform**.  This suggests to me that **Trainer isn't including the transform graph in the SavedModel that it creates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the SavedModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeldir: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Trainer/output/6/serving_model_dir/export/chicago-taxi/1561488174\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "models = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.MODEL)\n",
    "modelroot = os.path.join(models.URI.iloc[0], 'serving_model_dir', 'export', 'chicago-taxi')\n",
    "newest = str(sorted([int(f) for f in listdir(modelroot) if f.isdigit()])[-1])\n",
    "modeldir = os.path.join(modelroot, newest)\n",
    "\n",
    "print('modeldir: {}'.format(modeldir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the feature columns for the Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "_MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "\n",
    "_CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "_FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "_BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "_VOCAB_FEATURE_KEYS = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "_VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "_OOV_SIZE = 10\n",
    "\n",
    "# WEIRD: Since we're using the features created by Transform, we need to change the names to match what Transform names them\n",
    "def _transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def _transformed_names(keys):\n",
    "    return [_transformed_name(key) for key in keys]\n",
    "\n",
    "real_valued_columns = [\n",
    "    tf.feature_column.numeric_column(key, shape=(), default_value=0)\n",
    "    for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    tf.feature_column.categorical_column_with_identity(key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
    "    for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns += [\n",
    "    tf.feature_column.categorical_column_with_identity(\n",
    "        key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
    "    for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
    "]\n",
    "\n",
    "categorical_columns += [\n",
    "    tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
    "        key, num_buckets=num_buckets, default_value=0) for key, num_buckets in zip(\n",
    "        _transformed_names(_CATEGORICAL_FEATURE_KEYS), _MAX_CATEGORICAL_FEATURE_VALUES)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the trained Estimator from the SavedModel saved by Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0625 18:22:06.853214 140249631389504 estimator.py:1739] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmvcmzu9o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0625 18:22:06.856242 140249631389504 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpmvcmzu9o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e5d351518>, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpmvcmzu9o', '_log_step_count_steps': 100, '_evaluation_master': '', '_tf_random_seed': None, '_device_fn': None, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_task_id': 0, '_protocol': None, '_eval_distribute': None, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_train_distribute': None, '_task_type': 'worker', '_keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:22:06.859966 140249631389504 estimator.py:201] Using config: {'_num_worker_replicas': 1, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e5d351518>, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpmvcmzu9o', '_log_step_count_steps': 100, '_evaluation_master': '', '_tf_random_seed': None, '_device_fn': None, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_task_id': 0, '_protocol': None, '_eval_distribute': None, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_train_distribute': None, '_task_type': 'worker', '_keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Warm-starting from a SavedModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:22:06.862369 140249631389504 estimator.py:2292] Warm-starting from a SavedModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is a (<class 'tensorflow_estimator.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier'>)\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes in the first layer of the DNN\n",
    "first_dnn_layer_size = 100\n",
    "num_dnn_layers = 4\n",
    "dnn_decay_factor = 0.7\n",
    "\n",
    "hidden_units=[\n",
    "    max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
    "    for i in range(num_dnn_layers)\n",
    "]\n",
    "\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    linear_feature_columns=categorical_columns,\n",
    "    dnn_feature_columns=real_valued_columns,\n",
    "    dnn_hidden_units=hidden_units,\n",
    "    warm_start_from=modeldir)\n",
    "    \n",
    "print('model is a ({})'.format(type(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: Grab the schema created by Transform:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_root: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/:\n",
      "metadata/  transformed_metadata/  transform_fn/\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/metadata:\n",
      "schema.pbtxt\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/transformed_metadata:\n",
      "schema.pbtxt\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/transform_fn:\n",
      "assets/  saved_model.pb  variables/\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/transform_fn/assets:\n",
      "vocab_compute_and_apply_vocabulary_1_vocabulary\n",
      "vocab_compute_and_apply_vocabulary_vocabulary\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/transform_fn/variables:\n",
      "schema_uri: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/Transform/transform_output/4/transformed_metadata/schema.pbtxt\n",
      "example_gen_root: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1/train/\n",
      "schema_gen_root: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/SchemaGen/output/3/\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/SchemaGen/output/3/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "transform_root = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.TRANSFORMED_EXAMPLES).iloc[0].URI\n",
    "print('transform_root: {}'.format(transform_root))\n",
    "!ls -RF {transform_root}\n",
    "schema_uri = os.path.join(transform_root, 'transformed_metadata', 'schema.pbtxt')\n",
    "print('schema_uri: {}'.format(schema_uri))\n",
    "\n",
    "example_gen_root = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.EXAMPLES).iloc[0].URI\n",
    "print('example_gen_root: {}'.format(example_gen_root))\n",
    "schema_gen_root = store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.SCHEMA).iloc[0].URI\n",
    "print('schema_gen_root: {}'.format(schema_gen_root))\n",
    "\n",
    "original_schema_uri = os.path.join(schema_gen_root, 'schema.pbtxt')\n",
    "!ls -RF {original_schema_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'company_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'dropoff_census_tract_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'dropoff_community_area_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'dropoff_latitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'dropoff_longitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'fare_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'payment_type_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_census_tract_xf': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
      "    'pickup_community_area_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_latitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'pickup_longitude_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'tips_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_miles_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'trip_seconds_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
      "    'trip_start_day_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_start_hour_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
      "    'trip_start_month_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "Feature spec for original schema\n",
      "{   'company': VarLenFeature(dtype=tf.string),\n",
      "    'dropoff_census_tract': VarLenFeature(dtype=tf.float32),\n",
      "    'dropoff_community_area': VarLenFeature(dtype=tf.float32),\n",
      "    'dropoff_latitude': VarLenFeature(dtype=tf.float32),\n",
      "    'dropoff_longitude': VarLenFeature(dtype=tf.float32),\n",
      "    'fare': VarLenFeature(dtype=tf.float32),\n",
      "    'payment_type': VarLenFeature(dtype=tf.string),\n",
      "    'pickup_census_tract': VarLenFeature(dtype=tf.string),\n",
      "    'pickup_community_area': VarLenFeature(dtype=tf.int64),\n",
      "    'pickup_latitude': VarLenFeature(dtype=tf.float32),\n",
      "    'pickup_longitude': VarLenFeature(dtype=tf.float32),\n",
      "    'tips': VarLenFeature(dtype=tf.float32),\n",
      "    'trip_miles': VarLenFeature(dtype=tf.float32),\n",
      "    'trip_seconds': VarLenFeature(dtype=tf.float32),\n",
      "    'trip_start_day': VarLenFeature(dtype=tf.int64),\n",
      "    'trip_start_hour': VarLenFeature(dtype=tf.int64),\n",
      "    'trip_start_month': VarLenFeature(dtype=tf.int64),\n",
      "    'trip_start_timestamp': VarLenFeature(dtype=tf.int64)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_transform as tft\n",
    "from tfx.utils import io_utils\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "schema_utils = tft.tf_metadata.schema_utils\n",
    "schema_proto = io_utils.parse_pbtxt_file(file_name=schema_uri, message=schema_pb2.Schema())\n",
    "feature_spec, domains = schema_utils.schema_as_feature_spec(schema_proto)\n",
    "\n",
    "pp.pprint(feature_spec)\n",
    "\n",
    "original_feature_spec, original_domains = schema_utils.schema_as_feature_spec(io_utils.parse_pbtxt_file(file_name=original_schema_uri, message=schema_pb2.Schema()))\n",
    "\n",
    "print('Feature spec for original schema')\n",
    "pp.pprint(original_feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: Extract the tf.Examples from the tf.Records file created by Transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples_root: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1:\n",
      "eval/  train/\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1/eval:\n",
      "data_tfrecord-00000-of-00001.gz\n",
      "\n",
      "/tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1/train:\n",
      "data_tfrecord-00000-of-00001.gz\n",
      "examples_uri: /tmp/tmpxd30uz6t/testSimplePipeline/tfx/pipelines/chicago_taxi_simple/CsvExampleGen/examples/1/eval/data_tfrecord-00000-of-00001.gz\n",
      "\n",
      "raw_dataset: (<class 'tensorflow.python.data.ops.readers.TFRecordDatasetV1'>) <TFRecordDatasetV1 shapes: (), types: tf.string>\n",
      "WARNING:tensorflow:From /usr/local/google/home/zhitaoli/tfx-env/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0625 18:22:07.538847 140249631389504 deprecation.py:323] From /usr/local/google/home/zhitaoli/tfx-env/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_examples is a: <class 'tensorflow.python.data.ops.readers.TFRecordDatasetV1'>\n"
     ]
    }
   ],
   "source": [
    "# examples_root = transform_root.replace('transform_output', 'transformed_examples')\n",
    "examples_root = os.path.dirname(store.get_artifacts_of_type_df(tfx_utils.TFXArtifactTypes.EXAMPLES).iloc[0].URI)\n",
    "examples_root = os.path.dirname(examples_root)\n",
    "print('examples_root: {}'.format(examples_root))\n",
    "!ls -RF {examples_root}\n",
    "examples_uri = os.path.join(examples_root, 'eval', listdir(os.path.join(examples_root, 'eval'))[0])\n",
    "print('examples_uri: {}\\n'.format(examples_uri))\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset([examples_uri], compression_type='GZIP')\n",
    "print('raw_dataset: ({}) {}'.format(type(raw_dataset), raw_dataset))\n",
    "\n",
    "parsed_examples = []\n",
    "for ex in raw_dataset:\n",
    "    ex2 = tf.train.Example.FromString(ex.numpy())\n",
    "    parsed_examples.append(ex2)\n",
    "print('parsed_examples is a: {}'.format(type(raw_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEIRD: This should not work.  The model should be expecting the examples from ExampleGen, and the schema from SchemaGen.  We're giving it the transformed examples from Transform, and the transformed schema from Transform.  Something is fishy.**\n",
    "\n",
    "Now analyze the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3833e20602e0409889a45e03eb266165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_name': 'chicago_taxi', 'inference_address': '127.0.0.1:8500', 'are_sequence_examples'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/zhitaoli/tfx-env/lib/python3.5/site-packages/tensorflow_serving/apis/prediction_service_pb2.py:131: DeprecationWarning: beta_create_PredictionService_stub() method is deprecated. This method will be removed in near future versions of TF Serving. Please switch to GA gRPC API in prediction_service_pb2_grpc.\n",
      "  'prediction_service_pb2_grpc.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget\n",
    "\n",
    "tool_height_in_px = 1000\n",
    "\n",
    "# Old code: Setup the tool with the test examples and the trained classifier\n",
    "# config_builder = WitConfigBuilder(parsed_examples).set_estimator_and_feature_spec(\n",
    "#    model, original_feature_spec).set_label_vocab(['good_tipper', 'bad_tipper'])\n",
    "# WitWidget(config_builder, height=tool_height_in_px)\n",
    "\n",
    "\n",
    "# New code: Setup the tool with the test examples and the trained classifier\n",
    "config_builder = WitConfigBuilder(parsed_examples).set_inference_address(\n",
    "    \"127.0.0.1:8500\").set_model_name(\"chicago_taxi\").set_label_vocab(['good_tipper', 'bad_tipper'])\n",
    "WitWidget(config_builder, height=tool_height_in_px)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
